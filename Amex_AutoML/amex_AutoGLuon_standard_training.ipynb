{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cff6477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9787360",
   "metadata": {},
   "source": [
    "## Autogluon model\n",
    "This notebook is to carry out training for the Amex Default Prediction competition using AutoGluon as a starting AutoML modeling technique.\n",
    "\n",
    "We are creating a notebook specifically for training to avoid loading the large test dataset. Instead of using the CSVs provided by the competition, we use the same datasets but in parquet format shared by a user within the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e58aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "#from dtreeviz.trees import *\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import gc\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c82119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27c54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"amex-data-integer-dtypes-parquet-format/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3ef37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training dataset and the labels\n",
    "raw_data_df = pd.read_parquet(f'{PATH}/train.parquet')\n",
    "train_labels_df = pd.read_csv(f'{PATH}/train_labels.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c68454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>...</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4020185</th>\n",
       "      <td>b9cdf83627a38ebbfd8b8eb2c943f631fa18ea567319513808e1be5adfc5e9f6</td>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>0.806505</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239033</th>\n",
       "      <td>0b1ecd1e9fd72ebf6e3188c581e90de0210c1d3133b231adfe38a3e4d763e9b7</td>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>1.000348</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538306</th>\n",
       "      <td>191091b92233586dde76a982176a9f9e25a2c39640de2d820aa04a62df21ac14</td>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>0.861135</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865396</th>\n",
       "      <td>84405a24538c7f1c023c05e613e5a1332880034c594813c475afdcbe5a807425</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>0.603687</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375221</th>\n",
       "      <td>f8c0112a130a70118e9b4c3442516f2853bcd01b4fd7156dee22484acabff263</td>\n",
       "      <td>2017-12-21</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110185</th>\n",
       "      <td>3348d5b84899bcd7b7855e71c5fa1eaca8e8c864824800010c24b32caa34a9d8</td>\n",
       "      <td>2017-04-06</td>\n",
       "      <td>0.570044</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              customer_ID  \\\n",
       "4020185  b9cdf83627a38ebbfd8b8eb2c943f631fa18ea567319513808e1be5adfc5e9f6   \n",
       "239033   0b1ecd1e9fd72ebf6e3188c581e90de0210c1d3133b231adfe38a3e4d763e9b7   \n",
       "538306   191091b92233586dde76a982176a9f9e25a2c39640de2d820aa04a62df21ac14   \n",
       "2865396  84405a24538c7f1c023c05e613e5a1332880034c594813c475afdcbe5a807425   \n",
       "5375221  f8c0112a130a70118e9b4c3442516f2853bcd01b4fd7156dee22484acabff263   \n",
       "1110185  3348d5b84899bcd7b7855e71c5fa1eaca8e8c864824800010c24b32caa34a9d8   \n",
       "\n",
       "                S_2       P_2  D_39  ...  D_142  D_143     D_144  D_145  \n",
       "4020185  2018-02-14  0.806505     0  ...    NaN      0  0.007989      0  \n",
       "239033   2017-07-08  1.000348     0  ...    NaN      0  0.007258      0  \n",
       "538306   2017-12-20  0.861135     0  ...    NaN      0  0.003216      0  \n",
       "2865396  2017-11-11  0.603687     1  ...   0.14      1  0.009432      1  \n",
       "5375221  2017-12-21  0.993622     0  ...    NaN      0  0.003863      0  \n",
       "1110185  2017-04-06  0.570044     0  ...    NaN      0  0.008626      0  \n",
       "\n",
       "[6 rows x 190 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the inputs\n",
    "raw_data_df.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b521a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        customer_ID  target\n",
       "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a       0\n",
       "1  00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5       0\n",
       "2  00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1       0\n",
       "3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275237e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5531451, 190), (458913, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "raw_data_df.shape, train_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8d6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing values. This is not actually needed since AutoGluon fills missing values before training as well\n",
    "raw_data_df = raw_data_df.bfill(axis='rows').ffill(axis='rows')\n",
    "raw_data_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97507f",
   "metadata": {},
   "source": [
    "The input dataset contains multiple statements per customer, so to get a representative set of values we could either groupby and select get an aggregate across all rows (mean, median, percentiles) or simply select the latest statement (tail). The latest statement gives a good indication of the current state of the customer, so we use that here. \n",
    "\n",
    "A follow up would be to check if model performance gets better by computing an aggregate of the row values per customer instead of the latest one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "306edb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_data_df.groupby(['customer_ID'],as_index=False).tail(1)\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf222f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 189), (458913, 2), (5531451, 190))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, train_labels_df.shape, raw_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7313300a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['customer_ID', 'P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3',\n",
       "        'D_42',\n",
       "        ...\n",
       "        'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143',\n",
       "        'D_144', 'D_145'],\n",
       "       dtype='object', length=189),\n",
       " Index(['customer_ID', 'target'], dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns, train_labels_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b372feb",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We have been specified some columns as categorical variables in the problem statement. We need to ensure these are specified as strings even though their values are likely integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de14dc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '-1' '2']\n",
      "['2' '1' '3' '6' '4' '5' '7' '0' '-1']\n",
      "['1' '0' '-1']\n",
      "['0' '-1' '1']\n",
      "['5' '0' '2' '7' '3' '1' '4' '6' '-1']\n",
      "['0' '-1' '1']\n",
      "['2' '0' '1']\n",
      "['0' '3' '4' '1' '2' '5']\n",
      "['0' '2' '1' '3' '-1']\n",
      "['-1' '1' '0']\n",
      "['6' '3' '1' '5' '0' '2' '4' '-1']\n"
     ]
    }
   ],
   "source": [
    "# Convert specific columns to categorical variables\n",
    "obj_col = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "for col in obj_col:\n",
    "    train_df[col]=train_df[col].astype('int').astype('str')\n",
    "    print(train_df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1651ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(train_labels_df, how='inner', on=\"customer_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d39a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample subset of data for faster demo, try setting this to much larger values\n",
    "# Not using these in the fit method below. Using the whole training dataset\n",
    "#subsample_size = 544041  \n",
    "#train_data_sample = train_df.sample(n=subsample_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "655f68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {  # hyperparameters of each model type\n",
    "                   'GBM': {},\n",
    "                   'XGB': {},\n",
    "                   'FASTAI': {},\n",
    "                   'CAT': {},\n",
    "                   'NN_TORCH': {}, \n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0610d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220924_172812/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 14400s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220924_172812/\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.8.12\n",
      "Operating System:   Linux\n",
      "Train Data Rows:    458913\n",
      "Train Data Columns: 189\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    34811.0 MB\n",
      "\tTrain Data (Original)  Memory Usage: 827.85 MB (2.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['customer_ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['customer_ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 177 | ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', ...]\n",
      "\t\t('object', []) :  11 | ['D_63', 'D_64', 'D_66', 'D_68', 'B_30', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  11 | ['D_63', 'D_64', 'D_66', 'D_68', 'B_30', ...]\n",
      "\t\t('float', [])    : 177 | ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', ...]\n",
      "\t11.9s = Fit runtime\n",
      "\t188 features in original data used to generate 188 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 484.16 MB (1.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 13.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'recall'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 14386.32s of the 14386.31s of remaining time.\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.234855\tvalid_set's recall: 0.795207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.235203\tvalid_set's recall: 0.797967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7968\t = Validation score   (recall)\n",
      "\t375.29s\t = Training   runtime\n",
      "\t2.81s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 14002.72s of the 14002.72s of remaining time.\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t0.7897\t = Validation score   (recall)\n",
      "\t287.79s\t = Training   runtime\n",
      "\t1.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 13709.19s of the 13709.19s of remaining time.\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "No improvement since epoch 7: early stopping\n",
      "No improvement since epoch 7: early stopping\n",
      "No improvement since epoch 8: early stopping\n",
      "No improvement since epoch 8: early stopping\n",
      "No improvement since epoch 5: early stopping\n",
      "\t0.8411\t = Validation score   (recall)\n",
      "\t2152.35s\t = Training   runtime\n",
      "\t6.75s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 11544.7s of the 11544.7s of remaining time.\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.7918\t = Validation score   (recall)\n",
      "\t260.24s\t = Training   runtime\n",
      "\t6.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 11272.84s of the 11272.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.827\t = Validation score   (recall)\n",
      "\t810.4s\t = Training   runtime\n",
      "\t26.04s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 10450.73s of the 10450.72s of remaining time.\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S2F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S2F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S2F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7959\t = Validation score   (recall)\n",
      "\t658.27s\t = Training   runtime\n",
      "\t4.79s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 10161.59s of the 10161.59s of remaining time.\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\tTraining S2F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\tTraining S2F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\tTraining S2F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10778.5 Total: 14910.375\n",
      "\t0.7891\t = Validation score   (recall)\n",
      "\t590.04s\t = Training   runtime\n",
      "\t2.39s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 9855.05s of the 9855.04s of remaining time.\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "No improvement since epoch 8: early stopping\n",
      "No improvement since epoch 5: early stopping\n",
      "No improvement since epoch 2: early stopping\n",
      "No improvement since epoch 9: early stopping\n",
      "No improvement since epoch 8: early stopping\n",
      "No improvement since epoch 9: early stopping\n",
      "\t0.8424\t = Validation score   (recall)\n",
      "\t4272.91s\t = Training   runtime\n",
      "\t13.67s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 7723.45s of the 7723.44s of remaining time.\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.794\t = Validation score   (recall)\n",
      "\t543.29s\t = Training   runtime\n",
      "\t13.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 7429.54s of the 7429.54s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.8273\t = Validation score   (recall)\n",
      "\t1589.65s\t = Training   runtime\n",
      "\t51.02s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/20\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6641.61s of the 6641.61s of remaining time.\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S3F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S3F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S3F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S3F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S3F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S3F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S3F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S3F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7963\t = Validation score   (recall)\n",
      "\t952.76s\t = Training   runtime\n",
      "\t7.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6340.63s of the 6340.63s of remaining time.\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S3F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\tTraining S3F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\tTraining S3F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\tTraining S3F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\tTraining S3F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\tTraining S3F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\tTraining S3F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\tTraining S3F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "Warning: less than 75% gpu memory available for training. Free: 10776.5 Total: 14910.375\n",
      "\t0.7894\t = Validation score   (recall)\n",
      "\t869.86s\t = Training   runtime\n",
      "\t3.53s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 6056.45s of the 6056.45s of remaining time.\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 2: early stopping\n",
      "No improvement since epoch 5: early stopping\n",
      "No improvement since epoch 9: early stopping\n",
      "No improvement since epoch 4: early stopping\n",
      "\t0.8447\t = Validation score   (recall)\n",
      "\t6280.96s\t = Training   runtime\n",
      "\t20.39s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 4037.56s of the 4037.55s of remaining time.\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.7953\t = Validation score   (recall)\n",
      "\t884.43s\t = Training   runtime\n",
      "\t19.88s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3685.67s of the 3685.67s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.8314\t = Validation score   (recall)\n",
      "\t2341.56s\t = Training   runtime\n",
      "\t75.92s\t = Validation runtime\n",
      "Completed 3/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1438.63s of the 2924.46s of remaining time.\n",
      "\t0.8447\t = Validation score   (recall)\n",
      "\t130.12s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 11607.11s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220924_172812/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='target', eval_metric='recall',).fit(train_df,ag_args_fit={'num_gpus': 1}\n",
    "                                                                 ,hyperparameters=hyperparams\n",
    "                                                                 , time_limit=4*60*60,presets=[\"best_quality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7443db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  NeuralNetFastAI_BAG_L1   0.844675      20.389701  6280.960492               20.389701        6280.960492            1       True          3\n",
      "1     WeightedEnsemble_L2   0.844675      21.162452  6411.078235                0.772751         130.117743            2       True          6\n",
      "2   NeuralNetTorch_BAG_L1   0.831412      75.923898  2341.561825               75.923898        2341.561825            1       True          5\n",
      "3         LightGBM_BAG_L1   0.796319       6.998059   952.755348                6.998059         952.755348            1       True          1\n",
      "4          XGBoost_BAG_L1   0.795318      19.879759   884.434290               19.879759         884.434290            1       True          4\n",
      "5         CatBoost_BAG_L1   0.789376       3.534292   869.855051                3.534292         869.855051            1       True          2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>...</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>0.844675</td>\n",
       "      <td>20.389701</td>\n",
       "      <td>6280.960492</td>\n",
       "      <td>...</td>\n",
       "      <td>6280.960492</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.844675</td>\n",
       "      <td>21.162452</td>\n",
       "      <td>6411.078235</td>\n",
       "      <td>...</td>\n",
       "      <td>130.117743</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetTorch_BAG_L1</td>\n",
       "      <td>0.831412</td>\n",
       "      <td>75.923898</td>\n",
       "      <td>2341.561825</td>\n",
       "      <td>...</td>\n",
       "      <td>2341.561825</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.796319</td>\n",
       "      <td>6.998059</td>\n",
       "      <td>952.755348</td>\n",
       "      <td>...</td>\n",
       "      <td>952.755348</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost_BAG_L1</td>\n",
       "      <td>0.795318</td>\n",
       "      <td>19.879759</td>\n",
       "      <td>884.434290</td>\n",
       "      <td>...</td>\n",
       "      <td>884.434290</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost_BAG_L1</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>3.534292</td>\n",
       "      <td>869.855051</td>\n",
       "      <td>...</td>\n",
       "      <td>869.855051</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  score_val  pred_time_val     fit_time  ...  \\\n",
       "0  NeuralNetFastAI_BAG_L1   0.844675      20.389701  6280.960492  ...   \n",
       "1     WeightedEnsemble_L2   0.844675      21.162452  6411.078235  ...   \n",
       "2   NeuralNetTorch_BAG_L1   0.831412      75.923898  2341.561825  ...   \n",
       "3         LightGBM_BAG_L1   0.796319       6.998059   952.755348  ...   \n",
       "4          XGBoost_BAG_L1   0.795318      19.879759   884.434290  ...   \n",
       "5         CatBoost_BAG_L1   0.789376       3.534292   869.855051  ...   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0        6280.960492            1       True          3  \n",
       "1         130.117743            2       True          6  \n",
       "2        2341.561825            1       True          5  \n",
       "3         952.755348            1       True          1  \n",
       "4         884.434290            1       True          4  \n",
       "5         869.855051            1       True          2  \n",
       "\n",
       "[6 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = TabularPredictor.load(\"AutogluonModels/ag-20220924_172812/\")\n",
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771ec77",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0378a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictor = TabularPredictor.load(\"AutogluonModels/ag-20220924_172812/\")\n",
    "predictor = TabularPredictor.load(\"AutogluonModels/ag-20220924_024411/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa352958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2_FULL'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab9424b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_models</th>\n",
       "      <th>num_models_w_ancestors</th>\n",
       "      <th>memory_size</th>\n",
       "      <th>memory_size_w_ancestors</th>\n",
       "      <th>memory_size_min</th>\n",
       "      <th>memory_size_min_w_ancestors</th>\n",
       "      <th>num_ancestors</th>\n",
       "      <th>num_descendants</th>\n",
       "      <th>model_type</th>\n",
       "      <th>child_model_type</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>hyperparameters_fit</th>\n",
       "      <th>ag_args_fit</th>\n",
       "      <th>features</th>\n",
       "      <th>child_hyperparameters</th>\n",
       "      <th>child_hyperparameters_fit</th>\n",
       "      <th>child_ag_args_fit</th>\n",
       "      <th>ancestors</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.838043</td>\n",
       "      <td>138.328958</td>\n",
       "      <td>5516.244585</td>\n",
       "      <td>0.769108</td>\n",
       "      <td>101.599411</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>3472</td>\n",
       "      <td>48808468</td>\n",
       "      <td>3472</td>\n",
       "      <td>1011985</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>WeightedEnsembleModel</td>\n",
       "      <td>GreedyWeightedEnsembleModel</td>\n",
       "      <td>{'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]</td>\n",
       "      <td>{'ensemble_size': 100}</td>\n",
       "      <td>{'ensemble_size': 77}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetTorch_BAG_L1</td>\n",
       "      <td>0.836495</td>\n",
       "      <td>94.468498</td>\n",
       "      <td>3396.305953</td>\n",
       "      <td>94.468498</td>\n",
       "      <td>3396.305953</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>173</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48224884</td>\n",
       "      <td>48224884</td>\n",
       "      <td>1011985</td>\n",
       "      <td>1011985</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>TabularNeuralNetTorchModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'num_epochs': 500, 'epochs_wo_improve': 20, 'activation': 'relu', 'embedding_size_factor': 1.0, 'embed_exponent': 0.56, 'max_embedding_dim': 100, 'y_range': None, 'y_range_extend': 0.05, 'dropout_prob': 0.1, 'optimizer': 'adam', 'learning_rate': 0.0003, 'weight_decay': 1e-06, 'proc.embed_min_categories': 4, 'proc.impute_strategy': 'median', 'proc.max_category_levels': 100, 'proc.skew_threshold': 0.99, 'use_ngram_features': False, 'num_layers': 4, 'hidden_size': 128, 'max_batch_size': 512, 'use_batchnorm': False, 'loss_function': 'auto'}</td>\n",
       "      <td>{'batch_size': 256, 'num_epochs': 9}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>0.836007</td>\n",
       "      <td>43.091352</td>\n",
       "      <td>2018.339220</td>\n",
       "      <td>43.091352</td>\n",
       "      <td>2018.339220</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>580112</td>\n",
       "      <td>580112</td>\n",
       "      <td>17381</td>\n",
       "      <td>17381</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>NNFastAiTabularModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}</td>\n",
       "      <td>{'epochs': 30, 'best_epoch': 7}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.806039</td>\n",
       "      <td>26.050655</td>\n",
       "      <td>531.317570</td>\n",
       "      <td>26.050655</td>\n",
       "      <td>531.317570</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>236697</td>\n",
       "      <td>236697</td>\n",
       "      <td>10146</td>\n",
       "      <td>10146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>LGBModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'learning_rate': 0.05}</td>\n",
       "      <td>{'num_boost_round': 219}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost_BAG_L1</td>\n",
       "      <td>0.803262</td>\n",
       "      <td>42.486201</td>\n",
       "      <td>370.850180</td>\n",
       "      <td>42.486201</td>\n",
       "      <td>370.850180</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>173</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>350890</td>\n",
       "      <td>350890</td>\n",
       "      <td>12535</td>\n",
       "      <td>12535</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>XGBoostModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'n_estimators': 10000, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'binary:logistic', 'booster': 'gbtree', 'use_label_encoder': False}</td>\n",
       "      <td>{'n_estimators': 144}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.099546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.099546</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1183505</td>\n",
       "      <td>1183505</td>\n",
       "      <td>1183505</td>\n",
       "      <td>1183505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>XGBoostModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'n_estimators': 144, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'binary:logistic', 'booster': 'gbtree', 'use_label_encoder': False}</td>\n",
       "      <td>{'n_estimators': 144}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WeightedEnsemble_L2_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>273.585621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.599411</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3672</td>\n",
       "      <td>1841930</td>\n",
       "      <td>3672</td>\n",
       "      <td>1310616</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>WeightedEnsembleModel</td>\n",
       "      <td>GreedyWeightedEnsembleModel</td>\n",
       "      <td>{'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]</td>\n",
       "      <td>{'ensemble_size': 100}</td>\n",
       "      <td>{'ensemble_size': 77}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[NeuralNetTorch_BAG_L1_FULL, NeuralNetFastAI_BAG_L1_FULL]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NeuralNetTorch_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.152001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.152001</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1310616</td>\n",
       "      <td>1310616</td>\n",
       "      <td>1310616</td>\n",
       "      <td>1310616</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>TabularNeuralNetTorchModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'num_epochs': 9, 'epochs_wo_improve': 20, 'activation': 'relu', 'embedding_size_factor': 1.0, 'embed_exponent': 0.56, 'max_embedding_dim': 100, 'y_range': None, 'y_range_extend': 0.05, 'dropout_prob': 0.1, 'optimizer': 'adam', 'learning_rate': 0.0003, 'weight_decay': 1e-06, 'proc.embed_min_categories': 4, 'proc.impute_strategy': 'median', 'proc.max_category_levels': 100, 'proc.skew_threshold': 0.99, 'use_ngram_features': False, 'num_layers': 4, 'hidden_size': 128, 'max_batch_size': 512, 'use_batchnorm': False, 'loss_function': 'auto', 'batch_size': 256}</td>\n",
       "      <td>{'batch_size': 256, 'num_epochs': 0}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2_FULL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.834209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.834209</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>527642</td>\n",
       "      <td>527642</td>\n",
       "      <td>527642</td>\n",
       "      <td>527642</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>NNFastAiTabularModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 30, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'best_epoch': 7}</td>\n",
       "      <td>{'epochs': 30, 'best_epoch': 7}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2_FULL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.230482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.230482</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800686</td>\n",
       "      <td>800686</td>\n",
       "      <td>800686</td>\n",
       "      <td>800686</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>LGBModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...</td>\n",
       "      <td>{'learning_rate': 0.05, 'num_boost_round': 219}</td>\n",
       "      <td>{'num_boost_round': 219}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  score_val  pred_time_val     fit_time  \\\n",
       "0          WeightedEnsemble_L2   0.838043     138.328958  5516.244585   \n",
       "1        NeuralNetTorch_BAG_L1   0.836495      94.468498  3396.305953   \n",
       "2       NeuralNetFastAI_BAG_L1   0.836007      43.091352  2018.339220   \n",
       "3              LightGBM_BAG_L1   0.806039      26.050655   531.317570   \n",
       "4               XGBoost_BAG_L1   0.803262      42.486201   370.850180   \n",
       "5          XGBoost_BAG_L1_FULL        NaN            NaN    12.099546   \n",
       "6     WeightedEnsemble_L2_FULL        NaN            NaN   273.585621   \n",
       "7   NeuralNetTorch_BAG_L1_FULL        NaN            NaN   100.152001   \n",
       "8  NeuralNetFastAI_BAG_L1_FULL        NaN            NaN    71.834209   \n",
       "9         LightGBM_BAG_L1_FULL        NaN            NaN    11.230482   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.769108         101.599411            2      False   \n",
       "1               94.468498        3396.305953            1      False   \n",
       "2               43.091352        2018.339220            1      False   \n",
       "3               26.050655         531.317570            1      False   \n",
       "4               42.486201         370.850180            1      False   \n",
       "5                     NaN          12.099546            1       True   \n",
       "6                     NaN         101.599411            2       True   \n",
       "7                     NaN         100.152001            1       True   \n",
       "8                     NaN          71.834209            1       True   \n",
       "9                     NaN          11.230482            1       True   \n",
       "\n",
       "   fit_order  num_features  num_models  num_models_w_ancestors  memory_size  \\\n",
       "0          5             2           1                      97         3472   \n",
       "1          4           173          48                      48     48224884   \n",
       "2          2           173          48                      48       580112   \n",
       "3          1           173          48                      48       236697   \n",
       "4          3           173          48                      48       350890   \n",
       "5          8           173           1                       1      1183505   \n",
       "6         10             2           1                       3         3672   \n",
       "7          9           173           1                       1      1310616   \n",
       "8          7           173           1                       1       527642   \n",
       "9          6           173           1                       1       800686   \n",
       "\n",
       "   memory_size_w_ancestors  memory_size_min  memory_size_min_w_ancestors  \\\n",
       "0                 48808468             3472                      1011985   \n",
       "1                 48224884          1011985                      1011985   \n",
       "2                   580112            17381                        17381   \n",
       "3                   236697            10146                        10146   \n",
       "4                   350890            12535                        12535   \n",
       "5                  1183505          1183505                      1183505   \n",
       "6                  1841930             3672                      1310616   \n",
       "7                  1310616          1310616                      1310616   \n",
       "8                   527642           527642                       527642   \n",
       "9                   800686           800686                       800686   \n",
       "\n",
       "   num_ancestors  num_descendants             model_type  \\\n",
       "0              2                0  WeightedEnsembleModel   \n",
       "1              0                1   StackerEnsembleModel   \n",
       "2              0                1   StackerEnsembleModel   \n",
       "3              0                0   StackerEnsembleModel   \n",
       "4              0                0   StackerEnsembleModel   \n",
       "5              0                0   StackerEnsembleModel   \n",
       "6              2                0  WeightedEnsembleModel   \n",
       "7              0                1   StackerEnsembleModel   \n",
       "8              0                1   StackerEnsembleModel   \n",
       "9              0                0   StackerEnsembleModel   \n",
       "\n",
       "              child_model_type  \\\n",
       "0  GreedyWeightedEnsembleModel   \n",
       "1   TabularNeuralNetTorchModel   \n",
       "2         NNFastAiTabularModel   \n",
       "3                     LGBModel   \n",
       "4                 XGBoostModel   \n",
       "5                 XGBoostModel   \n",
       "6  GreedyWeightedEnsembleModel   \n",
       "7   TabularNeuralNetTorchModel   \n",
       "8         NNFastAiTabularModel   \n",
       "9                     LGBModel   \n",
       "\n",
       "                                                                                              hyperparameters  \\\n",
       "0  {'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n",
       "1  {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}   \n",
       "2  {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}   \n",
       "3  {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}   \n",
       "4  {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': False}   \n",
       "5   {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n",
       "6  {'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n",
       "7   {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n",
       "8   {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n",
       "9   {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n",
       "\n",
       "  hyperparameters_fit  \\\n",
       "0                  {}   \n",
       "1                  {}   \n",
       "2                  {}   \n",
       "3                  {}   \n",
       "4                  {}   \n",
       "5                  {}   \n",
       "6                  {}   \n",
       "7                  {}   \n",
       "8                  {}   \n",
       "9                  {}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                ag_args_fit  \\\n",
       "0   {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "1   {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "2   {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "3   {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "4   {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "5  {'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "6   {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "7  {'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "8  {'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "9  {'max_memory_usage_ratio': 1.15, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  features  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]   \n",
       "1  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "2  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "3  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "4  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "5  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]   \n",
       "7  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "8  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "9  [D_116, B_14, R_4, D_39, R_23, D_119, B_16, S_2.day, D_143, D_47, B_10, D_74, R_28, R_1, D_44, S_15, D_118, R_22, D_128, S_23, S_3, P_4, S_19, D_89, D_117, D_140, D_63, S_6, D_54, R_16, D_72, B_20, D_87, B_19, B_41, D_122, B_13, R_6, S_16, D_102, D_125, B_1, D_83, B_26, D_48, D_130, B_40, D_138, D_51, B_37, D_91, R_12, D_124, B_3, R_9, D_136, S_12, D_114, B_28, R_15, S_7, D_137, S_26, R_20, B_18, P_2, D_43, D_75, D_123, S_18, P_3, B_5, D_144, D_66, D_80, S_24, R_18, D_126, D_60, B_23, R_17, S_2.dayofweek, D_111, D_82, R_2, B_31, S_25, D_129, D_79, R_11, S_13, D_127, D_64, R_10, D_131, B_33...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              child_hyperparameters  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {'ensemble_size': 100}   \n",
       "1                   {'num_epochs': 500, 'epochs_wo_improve': 20, 'activation': 'relu', 'embedding_size_factor': 1.0, 'embed_exponent': 0.56, 'max_embedding_dim': 100, 'y_range': None, 'y_range_extend': 0.05, 'dropout_prob': 0.1, 'optimizer': 'adam', 'learning_rate': 0.0003, 'weight_decay': 1e-06, 'proc.embed_min_categories': 4, 'proc.impute_strategy': 'median', 'proc.max_category_levels': 100, 'proc.skew_threshold': 0.99, 'use_ngram_features': False, 'num_layers': 4, 'hidden_size': 128, 'max_batch_size': 512, 'use_batchnorm': False, 'loss_function': 'auto'}   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                     {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {'learning_rate': 0.05}   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                     {'n_estimators': 10000, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'binary:logistic', 'booster': 'gbtree', 'use_label_encoder': False}   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                       {'n_estimators': 144, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'binary:logistic', 'booster': 'gbtree', 'use_label_encoder': False}   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {'ensemble_size': 100}   \n",
       "7  {'num_epochs': 9, 'epochs_wo_improve': 20, 'activation': 'relu', 'embedding_size_factor': 1.0, 'embed_exponent': 0.56, 'max_embedding_dim': 100, 'y_range': None, 'y_range_extend': 0.05, 'dropout_prob': 0.1, 'optimizer': 'adam', 'learning_rate': 0.0003, 'weight_decay': 1e-06, 'proc.embed_min_categories': 4, 'proc.impute_strategy': 'median', 'proc.max_category_levels': 100, 'proc.skew_threshold': 0.99, 'use_ngram_features': False, 'num_layers': 4, 'hidden_size': 128, 'max_batch_size': 512, 'use_batchnorm': False, 'loss_function': 'auto', 'batch_size': 256}   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                        {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 30, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'best_epoch': 7}   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'learning_rate': 0.05, 'num_boost_round': 219}   \n",
       "\n",
       "              child_hyperparameters_fit  \\\n",
       "0                 {'ensemble_size': 77}   \n",
       "1  {'batch_size': 256, 'num_epochs': 9}   \n",
       "2       {'epochs': 30, 'best_epoch': 7}   \n",
       "3              {'num_boost_round': 219}   \n",
       "4                 {'n_estimators': 144}   \n",
       "5                 {'n_estimators': 144}   \n",
       "6                 {'ensemble_size': 77}   \n",
       "7  {'batch_size': 256, 'num_epochs': 0}   \n",
       "8       {'epochs': 30, 'best_epoch': 7}   \n",
       "9              {'num_boost_round': 219}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                child_ag_args_fit  \\\n",
       "0                                                         {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "1  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "2  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "3                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "4                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "5                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "6                                                         {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "7  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "8  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "9                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "\n",
       "                                                   ancestors  \\\n",
       "0            [NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]   \n",
       "1                                                         []   \n",
       "2                                                         []   \n",
       "3                                                         []   \n",
       "4                                                         []   \n",
       "5                                                         []   \n",
       "6  [NeuralNetTorch_BAG_L1_FULL, NeuralNetFastAI_BAG_L1_FULL]   \n",
       "7                                                         []   \n",
       "8                                                         []   \n",
       "9                                                         []   \n",
       "\n",
       "                  descendants  \n",
       "0                          []  \n",
       "1       [WeightedEnsemble_L2]  \n",
       "2       [WeightedEnsemble_L2]  \n",
       "3                          []  \n",
       "4                          []  \n",
       "5                          []  \n",
       "6                          []  \n",
       "7  [WeightedEnsemble_L2_FULL]  \n",
       "8  [WeightedEnsemble_L2_FULL]  \n",
       "9                          []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(predictor.leaderboard(extra_info=True, silent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba1e64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>...</th>\n",
       "      <th>child_hyperparameters_fit</th>\n",
       "      <th>child_ag_args_fit</th>\n",
       "      <th>ancestors</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.838043</td>\n",
       "      <td>138.328958</td>\n",
       "      <td>5516.244585</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ensemble_size': 77}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetTorch_BAG_L1</td>\n",
       "      <td>0.836495</td>\n",
       "      <td>94.468498</td>\n",
       "      <td>3396.305953</td>\n",
       "      <td>...</td>\n",
       "      <td>{'batch_size': 256, 'num_epochs': 9}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>0.836007</td>\n",
       "      <td>43.091352</td>\n",
       "      <td>2018.339220</td>\n",
       "      <td>...</td>\n",
       "      <td>{'epochs': 30, 'best_epoch': 7}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.806039</td>\n",
       "      <td>26.050655</td>\n",
       "      <td>531.317570</td>\n",
       "      <td>...</td>\n",
       "      <td>{'num_boost_round': 219}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost_BAG_L1</td>\n",
       "      <td>0.803262</td>\n",
       "      <td>42.486201</td>\n",
       "      <td>370.850180</td>\n",
       "      <td>...</td>\n",
       "      <td>{'n_estimators': 144}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.099546</td>\n",
       "      <td>...</td>\n",
       "      <td>{'n_estimators': 144}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WeightedEnsemble_L2_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>273.585621</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ensemble_size': 77}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}</td>\n",
       "      <td>[NeuralNetTorch_BAG_L1_FULL, NeuralNetFastAI_BAG_L1_FULL]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NeuralNetTorch_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.152001</td>\n",
       "      <td>...</td>\n",
       "      <td>{'batch_size': 256, 'num_epochs': 0}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2_FULL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.834209</td>\n",
       "      <td>...</td>\n",
       "      <td>{'epochs': 30, 'best_epoch': 7}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2_FULL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM_BAG_L1_FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.230482</td>\n",
       "      <td>...</td>\n",
       "      <td>{'num_boost_round': 219}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  score_val  pred_time_val     fit_time  ...  \\\n",
       "0          WeightedEnsemble_L2   0.838043     138.328958  5516.244585  ...   \n",
       "1        NeuralNetTorch_BAG_L1   0.836495      94.468498  3396.305953  ...   \n",
       "2       NeuralNetFastAI_BAG_L1   0.836007      43.091352  2018.339220  ...   \n",
       "3              LightGBM_BAG_L1   0.806039      26.050655   531.317570  ...   \n",
       "4               XGBoost_BAG_L1   0.803262      42.486201   370.850180  ...   \n",
       "5          XGBoost_BAG_L1_FULL        NaN            NaN    12.099546  ...   \n",
       "6     WeightedEnsemble_L2_FULL        NaN            NaN   273.585621  ...   \n",
       "7   NeuralNetTorch_BAG_L1_FULL        NaN            NaN   100.152001  ...   \n",
       "8  NeuralNetFastAI_BAG_L1_FULL        NaN            NaN    71.834209  ...   \n",
       "9         LightGBM_BAG_L1_FULL        NaN            NaN    11.230482  ...   \n",
       "\n",
       "              child_hyperparameters_fit  \\\n",
       "0                 {'ensemble_size': 77}   \n",
       "1  {'batch_size': 256, 'num_epochs': 9}   \n",
       "2       {'epochs': 30, 'best_epoch': 7}   \n",
       "3              {'num_boost_round': 219}   \n",
       "4                 {'n_estimators': 144}   \n",
       "5                 {'n_estimators': 144}   \n",
       "6                 {'ensemble_size': 77}   \n",
       "7  {'batch_size': 256, 'num_epochs': 0}   \n",
       "8       {'epochs': 30, 'best_epoch': 7}   \n",
       "9              {'num_boost_round': 219}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                child_ag_args_fit  \\\n",
       "0                                                         {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "1  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "2  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "3                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "4                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "5                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "6                                                         {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'drop_unique': False}   \n",
       "7  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "8  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "9                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'num_gpus': 1}   \n",
       "\n",
       "                                                   ancestors  \\\n",
       "0            [NeuralNetFastAI_BAG_L1, NeuralNetTorch_BAG_L1]   \n",
       "1                                                         []   \n",
       "2                                                         []   \n",
       "3                                                         []   \n",
       "4                                                         []   \n",
       "5                                                         []   \n",
       "6  [NeuralNetTorch_BAG_L1_FULL, NeuralNetFastAI_BAG_L1_FULL]   \n",
       "7                                                         []   \n",
       "8                                                         []   \n",
       "9                                                         []   \n",
       "\n",
       "                  descendants  \n",
       "0                          []  \n",
       "1       [WeightedEnsemble_L2]  \n",
       "2       [WeightedEnsemble_L2]  \n",
       "3                          []  \n",
       "4                          []  \n",
       "5                          []  \n",
       "6                          []  \n",
       "7  [WeightedEnsemble_L2_FULL]  \n",
       "8  [WeightedEnsemble_L2_FULL]  \n",
       "9                          []  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(extra_info=True, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7cea0",
   "metadata": {},
   "source": [
    "## Notes\n",
    "This notebook carries out the training process and we intentionally separate the testing and generation of predictions in to a separate notebook. Loading the testing file and then generating the predictions take a fair amount of time and memory, so keeping them separate helps to make things run better. \n",
    "\n",
    "Inferences using AutoGluon also allows for a separate training and testing process since the models are all written to disk and can be simply loaded later/elsewhere. This makes it very flexible to be run on different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2c506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
